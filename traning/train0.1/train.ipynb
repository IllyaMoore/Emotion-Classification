{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f86702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import ast\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d21446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\illya\\AppData\\Local\\Temp\\ipykernel_15744\\2144087589.py:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  df = pd.read_csv(\"C:\\Emotion Classification\\\\traning\\in_model_data\\data.csv\", sep=',')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>example_very_unclear</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26201</th>\n",
       "      <td>[41470, 0, 115, 69, 967, 332, 395, 131, 25202,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50582</th>\n",
       "      <td>[41470, 0, 2658, 41477, 825, 21524, 41488]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "26201  [41470, 0, 115, 69, 967, 332, 395, 131, 25202,...   \n",
       "50582         [41470, 0, 2658, 41477, 825, 21524, 41488]   \n",
       "\n",
       "       example_very_unclear  admiration  amusement  anger  annoyance  \\\n",
       "26201                     0           0          0      0          0   \n",
       "50582                     0           0          0      0          0   \n",
       "\n",
       "       approval  caring  confusion  curiosity  ...  love  nervousness  \\\n",
       "26201         1       0          0          0  ...     1            0   \n",
       "50582         0       0          0          0  ...     0            0   \n",
       "\n",
       "       optimism  pride  realization  relief  remorse  sadness  surprise  \\\n",
       "26201         0      0            0       0        0        0         0   \n",
       "50582         0      0            0       0        0        0         0   \n",
       "\n",
       "       neutral  \n",
       "26201        0  \n",
       "50582        0  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\Emotion Classification\\\\traning\\in_model_data\\data.csv\", sep=',')\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab84f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(ast.literal_eval)\n",
    "list_of_lists = df['text'].apply(lambda x: [int(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "91d05919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['text'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "61f88c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in list_of_lists]\n",
    "X = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "y = df.drop(columns=['text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8ef3468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ccd9dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "19970d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([56000, 142]),\n",
       " torch.Size([56000, 29]),\n",
       " torch.Size([14000, 142]),\n",
       " torch.Size([14000, 29]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847664a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Використовується пристрій: cpu\n",
      "Розмір тренувальних даних: torch.Size([56000, 142])\n",
      "Розмір тренувальних міток: torch.Size([56000, 29])\n",
      "Мінімальне значення в X_train: 0\n",
      "Максимальне значення в X_train: 41991\n",
      "Тип даних X_train: torch.int64\n",
      "Тип даних y_train: torch.int64\n",
      "\n",
      "Максимальний індекс: 41991\n",
      "Розмір словника: 41992\n",
      "Фінальний розмір словника: 41992\n",
      "Модель створена з 6304029 параметрами\n",
      "Початок тренування...\n",
      "Epoch [1/5], Batch [100/875], Loss: 0.1570\n",
      "Epoch [1/5], Batch [200/875], Loss: 0.1690\n",
      "Epoch [1/5], Batch [300/875], Loss: 0.1641\n",
      "Epoch [1/5], Batch [400/875], Loss: 0.1626\n",
      "Epoch [1/5], Batch [500/875], Loss: 0.1582\n",
      "Epoch [1/5], Batch [600/875], Loss: 0.1479\n",
      "Epoch [1/5], Batch [700/875], Loss: 0.1612\n",
      "Epoch [1/5], Batch [800/875], Loss: 0.1690\n",
      "Epoch [1/5]:\n",
      "  Train Loss: 0.1613, Train Acc: 0.9575\n",
      "  Val Loss: 0.1554, Val Acc: 0.9586\n",
      "  Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch [2/5], Batch [100/875], Loss: 0.1621\n",
      "Epoch [2/5], Batch [200/875], Loss: 0.1548\n",
      "Epoch [2/5], Batch [300/875], Loss: 0.1469\n",
      "Epoch [2/5], Batch [400/875], Loss: 0.1468\n",
      "Epoch [2/5], Batch [500/875], Loss: 0.1536\n",
      "Epoch [2/5], Batch [600/875], Loss: 0.1654\n",
      "Epoch [2/5], Batch [700/875], Loss: 0.1771\n",
      "Epoch [2/5], Batch [800/875], Loss: 0.1599\n",
      "Epoch [2/5]:\n",
      "  Train Loss: 0.1556, Train Acc: 0.9588\n",
      "  Val Loss: 0.1552, Val Acc: 0.9586\n",
      "  Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch [3/5], Batch [100/875], Loss: 0.1633\n",
      "Epoch [3/5], Batch [200/875], Loss: 0.1429\n",
      "Epoch [3/5], Batch [300/875], Loss: 0.1507\n",
      "Epoch [3/5], Batch [400/875], Loss: 0.1574\n",
      "Epoch [3/5], Batch [500/875], Loss: 0.1522\n",
      "Epoch [3/5], Batch [600/875], Loss: 0.1581\n",
      "Epoch [3/5], Batch [700/875], Loss: 0.1600\n",
      "Epoch [3/5], Batch [800/875], Loss: 0.1310\n",
      "Epoch [3/5]:\n",
      "  Train Loss: 0.1552, Train Acc: 0.9588\n",
      "  Val Loss: 0.1552, Val Acc: 0.9586\n",
      "  Learning Rate: 0.001000\n",
      "------------------------------------------------------------\n",
      "Epoch [4/5], Batch [100/875], Loss: 0.1440\n",
      "Epoch [4/5], Batch [200/875], Loss: 0.1616\n",
      "Epoch [4/5], Batch [300/875], Loss: 0.1468\n",
      "Epoch [4/5], Batch [400/875], Loss: 0.1651\n",
      "Epoch [4/5], Batch [500/875], Loss: 0.1527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Gradient clipping для стабільності\u001b[39;00m\n\u001b[0;32m    159\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Перевірка доступності GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Використовується пристрій: {device}\")\n",
    "\n",
    "# Перевірка даних\n",
    "print(f\"Розмір тренувальних даних: {X_train.shape}\")\n",
    "print(f\"Розмір тренувальних міток: {y_train.shape}\")\n",
    "print(f\"Мінімальне значення в X_train: {X_train.min().item()}\")\n",
    "print(f\"Максимальне значення в X_train: {X_train.max().item()}\")\n",
    "print(f\"Тип даних X_train: {X_train.dtype}\")\n",
    "print(f\"Тип даних y_train: {y_train.dtype}\")\n",
    "print()\n",
    "\n",
    "class GeomotionLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, output_dim, dropout=0.3):\n",
    "        super(GeomotionLSTM, self).__init__()\n",
    "        \n",
    "        # Embedding шар для перетворення індексів на вектори\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # LSTM шари\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Dropout для регуляризації\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Повністю підключений шар для класифікації\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Sigmoid для багатокласової класифікації\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Використовуємо останній вихід LSTM\n",
    "        last_output = lstm_out[:, -1, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        dropped = self.dropout(last_output)\n",
    "        \n",
    "        # Класифікація\n",
    "        output = self.fc(dropped)  # (batch_size, output_dim)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Визначення реального розміру словника з ваших даних\n",
    "max_index = int(X_train.max().item())\n",
    "VOCAB_SIZE = max_index + 1  # +1 тому що індексація з 0\n",
    "print(f\"Максимальний індекс: {max_index}\")\n",
    "print(f\"Розмір словника: {VOCAB_SIZE}\")\n",
    "\n",
    "# Опційно: обрізання великих індексів якщо потрібно\n",
    "# Розкоментуйте якщо VOCAB_SIZE занадто великий\n",
    "# MAX_VOCAB_SIZE = 50000\n",
    "# if VOCAB_SIZE > MAX_VOCAB_SIZE:\n",
    "#     print(f\"Обрізання словника до {MAX_VOCAB_SIZE}\")\n",
    "#     X_train = torch.clamp(X_train, max=MAX_VOCAB_SIZE-1)\n",
    "#     X_test = torch.clamp(X_test, max=MAX_VOCAB_SIZE-1)\n",
    "#     VOCAB_SIZE = MAX_VOCAB_SIZE\n",
    "\n",
    "print(f\"Фінальний розмір словника: {VOCAB_SIZE}\")\n",
    "\n",
    "# Гіперпараметри моделі\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_DIM = 29  # 29 класів згідно з вашими даними\n",
    "DROPOUT = 0.3\n",
    "\n",
    "# Параметри тренування\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Створення моделі\n",
    "model = GeomotionLSTM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(f\"Модель створена з {sum(p.numel() for p in model.parameters())} параметрами\")\n",
    "\n",
    "# Створення DataLoader'ів\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Функція втрат і оптимізатор\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy для багатокласової класифікації\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "# Scheduler для зменшення learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Функція для обчислення accuracy\n",
    "def calculate_accuracy(outputs, targets, threshold=0.5):\n",
    "    predicted = (outputs > threshold).float()\n",
    "    correct = (predicted == targets).float()\n",
    "    accuracy = correct.sum() / (correct.size(0) * correct.size(1))\n",
    "    return accuracy.item()\n",
    "\n",
    "# Списки для збереження метрик\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"Початок тренування...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Тренування\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device).float()\n",
    "        \n",
    "        # Обнулення градієнтів\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping для стабільності\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Оновлення параметрів\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Збереження метрик\n",
    "        train_loss += loss.item()\n",
    "        train_acc += calculate_accuracy(outputs, targets)\n",
    "        \n",
    "        # Виведення прогресу кожні 100 батчів\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Batch [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Валідація\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device).float()\n",
    "            \n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += calculate_accuracy(outputs, targets)\n",
    "    \n",
    "    # Обчислення середніх метрик\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_acc / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    avg_val_acc = val_acc / len(test_loader)\n",
    "    \n",
    "    # Збереження метрик\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_acc)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_acc)\n",
    "    \n",
    "    # Оновлення learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}]:')\n",
    "    print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.4f}')\n",
    "    print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {avg_val_acc:.4f}')\n",
    "    print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print('-' * 60)\n",
    "\n",
    "# Збереження моделі\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_losses': train_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'hyperparameters': {\n",
    "        'vocab_size': VOCAB_SIZE,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'hidden_dim': HIDDEN_DIM,\n",
    "        'num_layers': NUM_LAYERS,\n",
    "        'output_dim': OUTPUT_DIM,\n",
    "        'dropout': DROPOUT\n",
    "    }\n",
    "}, 'geomotion_lstm_model.pth')\n",
    "\n",
    "print(\"Модель збережена!\")\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(OUTPUT_DIM):\n",
    "    acc = accuracy_score(all_targets[:, i], all_predictions[:, i])\n",
    "    class_accuracies.append(acc)\n",
    "    print(f'Клас {i}: Accuracy = {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8cca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
