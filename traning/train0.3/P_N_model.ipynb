{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc891b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "491d8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\illya\\AppData\\Local\\Temp\\ipykernel_21808\\4134280587.py:17: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  TOKENIZER = \"C:\\Emotion Classification\\\\traning\\\\vocab\\\\tokenizer0.2.json\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "P_df = pd.read_csv(\"C:\\\\Emotion Classification\\\\treeModelsData\\\\positive_emotions.csv\")\n",
    "N_df = pd.read_csv(\"C:\\\\Emotion Classification\\\\treeModelsData\\\\negative_emotions.csv\")\n",
    "\n",
    "TOKENIZER = \"C:\\Emotion Classification\\\\traning\\\\vocab\\\\tokenizer0.2.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER)\n",
    "\n",
    "batchsize = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "05ba71ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43332, 14)"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.shape[0] + N_df.shape[0], P_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "a8a73b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          object\n",
       "admiration     int64\n",
       "amusement      int64\n",
       "approval       int64\n",
       "caring         int64\n",
       "curiosity      int64\n",
       "desire         int64\n",
       "excitement     int64\n",
       "gratitude      int64\n",
       "joy            int64\n",
       "love           int64\n",
       "optimism       int64\n",
       "pride          int64\n",
       "relief         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "6fa9d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_df['text'] = P_df['text'].str.lower()\n",
    "N_df['text'] = N_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "9682cd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>ohhh. [name] is my absolute favorite person ev...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  admiration  \\\n",
       "2864  ohhh. [name] is my absolute favorite person ev...           1   \n",
       "\n",
       "      amusement  approval  caring  curiosity  desire  excitement  gratitude  \\\n",
       "2864          0         0       0          0       0           0          0   \n",
       "\n",
       "      joy  love  optimism  pride  relief  \n",
       "2864    1     0         0      0       0  "
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "f3633530",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(P_df.shape[0] * 0.8)\n",
    "split_index\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "cbd615da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 128)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128\n",
    "vocab_size, embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "4763c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_text(data, split_ratio=0.8):\n",
    "    data = data.copy()\n",
    "    \n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: (tokenizer.encode(x)).ids)\n",
    "    \n",
    "    # Мітки\n",
    "    y = data.drop(columns=['text'])\n",
    "    \n",
    "    from sklearn.utils import shuffle\n",
    "    data = shuffle(data, random_state=42)\n",
    "    \n",
    "    y_main = y.values.argmax(axis=1)\n",
    "    data['main_label'] = y_main\n",
    "    # sort\n",
    "    data = data.sort_values('main_label').reset_index(drop=True)\n",
    "    data = data.drop(columns=['main_label'])\n",
    "    \n",
    "    # тензори\n",
    "    list_of_lists = data['text'].tolist()\n",
    "    tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in list_of_lists]\n",
    "    X = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    y = torch.tensor(data.drop(columns=['text']).values, dtype=torch.float32)\n",
    "    \n",
    "    # Split\n",
    "    split_index = int(len(data) * split_ratio)\n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    \n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "    \n",
    "X_train, y_train, X_test, y_test = pipeline_text(N_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "4ebace2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger             2363\n",
      "annoyance         3885\n",
      "confusion         1873\n",
      "disappointment    2313\n",
      "disapproval       3278\n",
      "disgust           1438\n",
      "embarrassment      690\n",
      "fear               871\n",
      "grief              187\n",
      "nervousness        495\n",
      "remorse            668\n",
      "sadness           1772\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "emotion_counts = N_df.drop(columns=['text']).sum()\n",
    "print(emotion_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a1705aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13512, 41]),\n",
       " torch.Size([13512, 12]),\n",
       " torch.Size([3378, 41]),\n",
       " torch.Size([3378, 12]),\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, type(X_train), type(y_train), type(X_test), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "d45b67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "003ba6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = X_train.shape[1]\n",
    "input_len = X_train.shape[1]\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "num_classes = y_train.shape[1]\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "0183a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_len, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)\n",
    "#         self.output_layer = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         hidden_size = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "#         cell_state = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "#         out, _ = self.lstm(X, (hidden_size, cell_state))\n",
    "#         out = self.output_layer(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_size, \n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, \n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size // 2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size // 4)\n",
    "        self.dropout3 = nn.Dropout(dropout * 0.5)\n",
    "        \n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "\n",
    "        out = lstm_out.mean(dim=1)\n",
    "        \n",
    "\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout3(out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "12e77b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMModel(input_len, hidden_size, num_layers, num_classes)\n",
    "model = LSTMModel(vocab_size, hidden_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "c8b5bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "90a3faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_dataloader, loss_func):\n",
    "    total_step = len(train_dataloader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (text_r, lables) in enumerate(train_dataloader):\n",
    "\n",
    "            outputs = model(text_r)\n",
    "            loss = loss_func(outputs, lables)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch + 1}/{total_step}], Loss: {loss.item():.4f}, Accuracy: {torch.sum(torch.argmax(outputs, dim=1) == torch.argmax(lables, dim=1)).item() / len(lables):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "91e612a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/226], Loss: 0.3232, Accuracy: 0.1667\n",
      "Epoch [1/10], Step [200/226], Loss: 0.2998, Accuracy: 0.2000\n",
      "Epoch [2/10], Step [100/226], Loss: 0.2786, Accuracy: 0.2667\n",
      "Epoch [2/10], Step [200/226], Loss: 0.2730, Accuracy: 0.3667\n",
      "Epoch [3/10], Step [100/226], Loss: 0.3057, Accuracy: 0.3333\n",
      "Epoch [3/10], Step [200/226], Loss: 0.3172, Accuracy: 0.2333\n",
      "Epoch [4/10], Step [100/226], Loss: 0.2261, Accuracy: 0.5000\n",
      "Epoch [4/10], Step [200/226], Loss: 0.2582, Accuracy: 0.3667\n",
      "Epoch [5/10], Step [100/226], Loss: 0.2509, Accuracy: 0.4500\n",
      "Epoch [5/10], Step [200/226], Loss: 0.2371, Accuracy: 0.3667\n",
      "Epoch [6/10], Step [100/226], Loss: 0.2232, Accuracy: 0.4333\n",
      "Epoch [6/10], Step [200/226], Loss: 0.2214, Accuracy: 0.4000\n",
      "Epoch [7/10], Step [100/226], Loss: 0.2071, Accuracy: 0.5000\n",
      "Epoch [7/10], Step [200/226], Loss: 0.2002, Accuracy: 0.5167\n",
      "Epoch [8/10], Step [100/226], Loss: 0.2230, Accuracy: 0.4833\n",
      "Epoch [8/10], Step [200/226], Loss: 0.2213, Accuracy: 0.4500\n",
      "Epoch [9/10], Step [100/226], Loss: 0.1956, Accuracy: 0.5167\n",
      "Epoch [9/10], Step [200/226], Loss: 0.1705, Accuracy: 0.5167\n",
      "Epoch [10/10], Step [100/226], Loss: 0.1667, Accuracy: 0.5833\n",
      "Epoch [10/10], Step [200/226], Loss: 0.1926, Accuracy: 0.6000\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=num_epochs, model=model, train_dataloader=train_dataloader, loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6de9d",
   "metadata": {},
   "source": [
    "Epoch [1/5], Step [100/353], Loss: 0.2903, Accuracy: 0.2667\n",
    "Epoch [1/5], Step [200/353], Loss: 0.2692, Accuracy: 0.3167\n",
    "Epoch [1/5], Step [300/353], Loss: 0.2249, Accuracy: 0.4000\n",
    "Epoch [2/5], Step [100/353], Loss: 0.2186, Accuracy: 0.4500\n",
    "Epoch [2/5], Step [200/353], Loss: 0.2328, Accuracy: 0.4667\n",
    "Epoch [2/5], Step [300/353], Loss: 0.2460, Accuracy: 0.4167\n",
    "Epoch [3/5], Step [100/353], Loss: 0.2194, Accuracy: 0.4333\n",
    "Epoch [3/5], Step [200/353], Loss: 0.1846, Accuracy: 0.5500\n",
    "Epoch [3/5], Step [300/353], Loss: 0.2377, Accuracy: 0.4667\n",
    "Epoch [4/5], Step [100/353], Loss: 0.1986, Accuracy: 0.6167\n",
    "Epoch [4/5], Step [200/353], Loss: 0.2355, Accuracy: 0.5333\n",
    "Epoch [4/5], Step [300/353], Loss: 0.2356, Accuracy: 0.3500\n",
    "Epoch [5/5], Step [100/353], Loss: 0.2068, Accuracy: 0.4667\n",
    "Epoch [5/5], Step [200/353], Loss: 0.1771, Accuracy: 0.6333\n",
    "Epoch [5/5], Step [300/353], Loss: 0.2150, Accuracy: 0.5667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "3bdafdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ahaha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text\n",
       "0  ahaha"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"ahaha\"\n",
    "d = {'text': [text]}\n",
    "test_df = pd.DataFrame(data=d)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "50416fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_text_for_inference(df=test_df, tokenizer=tokenizer):\n",
    "\n",
    "    df = df.copy()\n",
    "    tokenized = df['text'].apply(lambda x: tokenizer.encode(x).ids)\n",
    "    tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in tokenized.tolist()]\n",
    "    X = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "67a923f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_text_for_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "26c6840b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10687]])"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "e03bca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_new)\n",
    "    probs = torch.sigmoid(outputs)\n",
    "    preds = (probs > 0.5).int()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "b825f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = preds.tolist()\n",
    "\n",
    "for i in range(len(preds_list[0])):\n",
    "    if preds_list[0][i] == 1:\n",
    "        result = preds_list[0].index(preds_list[0][i]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "de2dff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admiration'"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.columns[result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "6e8613cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'admiration', 'amusement', 'approval', 'caring', 'curiosity',\n",
       "       'desire', 'excitement', 'gratitude', 'joy', 'love', 'optimism', 'pride',\n",
       "       'relief'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "181cc268",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\illya\\AppData\\Local\\Temp\\ipykernel_21808\\2112407754.py:1: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  torch.save(model.state_dict(), 'C:\\Emotion Classification\\\\traning\\\\train0.3\\models\\\\Nmodel.pth')\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'C:\\Emotion Classification\\\\traning\\\\train0.3\\models\\\\Nmodel.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
