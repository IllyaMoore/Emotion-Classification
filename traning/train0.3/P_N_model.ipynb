{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc891b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "491d8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\illya\\AppData\\Local\\Temp\\ipykernel_21808\\684743216.py:16: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  TOKENIZER = \"C:\\Emotion Classification\\\\traning\\\\vocab\\\\tokenizer0.2.json\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sentencepiece as spm\n",
    "from tokenizers import Tokenizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "P_df = pd.read_csv(\"C:\\\\Emotion Classification\\\\treeModelsData\\\\positive_emotions.csv\")\n",
    "N_df = pd.read_csv(\"C:\\\\Emotion Classification\\\\treeModelsData\\\\negative_emotions.csv\")\n",
    "\n",
    "TOKENIZER = \"C:\\Emotion Classification\\\\traning\\\\vocab\\\\tokenizer0.2.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER)\n",
    "\n",
    "batchsize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "05ba71ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43332, 14)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.shape[0] + N_df.shape[0], P_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a8a73b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          object\n",
       "admiration     int64\n",
       "amusement      int64\n",
       "approval       int64\n",
       "caring         int64\n",
       "curiosity      int64\n",
       "desire         int64\n",
       "excitement     int64\n",
       "gratitude      int64\n",
       "joy            int64\n",
       "love           int64\n",
       "optimism       int64\n",
       "pride          int64\n",
       "relief         int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6fa9d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_df['text'] = P_df['text'].str.lower()\n",
    "N_df['text'] = N_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9682cd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>excitement</th>\n",
       "      <th>gratitude</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>relief</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>ughh same. love the finish of it just wish it ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  admiration  \\\n",
       "16608  ughh same. love the finish of it just wish it ...           0   \n",
       "\n",
       "       amusement  approval  caring  curiosity  desire  excitement  gratitude  \\\n",
       "16608          0         0       0          0       0           0          0   \n",
       "\n",
       "       joy  love  optimism  pride  relief  \n",
       "16608    0     1         0      0       0  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f3633530",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_index = int(P_df.shape[0] * 0.8)\n",
    "split_index\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd615da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 720,   51,  681, 1141,   18])\n",
      "tensor([ 720,   51,  681, 1141,   18,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n"
     ]
    }
   ],
   "source": [
    "def pipeline_text(data):\n",
    "    data = data.copy()\n",
    "    \n",
    "    data[\"text\"] = data[\"text\"].apply(lambda x: (tokenizer.encode(x)).ids)\n",
    "    y = data.drop(columns=['text'])\n",
    "    \n",
    "    list_of_lists = data['text'].tolist()\n",
    "    tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in list_of_lists]\n",
    "    print(tensor_list[0])\n",
    "    X = pad_sequence(tensor_list, batch_first=True, padding_value=0)\n",
    "    print(X[0])\n",
    "    X = torch.tensor(nn.Embedding(vocab_size, ))\n",
    "\n",
    "    y = torch.tensor(y.values, dtype=torch.float32)\n",
    "    \n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "    \n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "X_train, y_train, X_test, y_test = pipeline_text(P_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a1705aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21153, 62]),\n",
       " torch.Size([21153, 13]),\n",
       " torch.Size([5289, 62]),\n",
       " torch.Size([5289, 13]),\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Tensor,\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, type(X_train), type(y_train), type(X_test), type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d45b67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ba6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = X_train.shape[1]\n",
    "input_len = X_train.shape[1]\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = y_train.shape[1]\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0183a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_len, hidden_size, num_layers, num_classes):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.num_layers = num_layers\n",
    "#         self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)\n",
    "#         self.output_layer = nn.Linear(hidden_size, num_classes) \n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         hidden_size = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "#         cell_state = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "#         out, _ = self.lstm(X, (hidden_size, cell_state))\n",
    "#         out = self.output_layer(out[:, -1, :])\n",
    "#         return out\n",
    "    \n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        out, _ = self.lstm(X)\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "12e77b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMModel(input_len, hidden_size, num_layers, num_classes)\n",
    "model = LSTMModel(vocab_size, hidden_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c8b5bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "90a3faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_dataloader, loss_func):\n",
    "    total_step = len(train_dataloader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (text_r, lables) in enumerate(train_dataloader):\n",
    "\n",
    "            outputs = model(text_r)\n",
    "            loss = loss_func(outputs, lables)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{batch + 1}/{total_step}], Loss: {loss.item():.4f}, Accuracy: {torch.sum(torch.argmax(outputs, dim=1) == torch.argmax(lables, dim=1)).item() / len(lables):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "91e612a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/212], Loss: 2.8750, Accuracy: 0.1500\n",
      "Epoch [1/5], Step [200/212], Loss: 3.1793, Accuracy: 0.1400\n",
      "Epoch [2/5], Step [100/212], Loss: 2.7031, Accuracy: 0.1900\n",
      "Epoch [2/5], Step [200/212], Loss: 2.9725, Accuracy: 0.1900\n",
      "Epoch [3/5], Step [100/212], Loss: 2.8838, Accuracy: 0.1500\n",
      "Epoch [3/5], Step [200/212], Loss: 2.9260, Accuracy: 0.2100\n",
      "Epoch [4/5], Step [100/212], Loss: 2.7821, Accuracy: 0.2300\n",
      "Epoch [4/5], Step [200/212], Loss: 2.9442, Accuracy: 0.2100\n",
      "Epoch [5/5], Step [100/212], Loss: 2.7177, Accuracy: 0.1800\n",
      "Epoch [5/5], Step [200/212], Loss: 2.8679, Accuracy: 0.1900\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=num_epochs, model=model, train_dataloader=train_dataloader, loss_func=loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6de9d",
   "metadata": {},
   "source": [
    "Epoch [1/5], Step [100/212], Loss: 3.0014, Accuracy: 0.1600\n",
    "Epoch [1/5], Step [200/212], Loss: 3.0891, Accuracy: 0.1600\n",
    "Epoch [2/5], Step [100/212], Loss: 3.0311, Accuracy: 0.1800\n",
    "Epoch [2/5], Step [200/212], Loss: 3.1607, Accuracy: 0.2200\n",
    "Epoch [3/5], Step [100/212], Loss: 2.8715, Accuracy: 0.2000\n",
    "Epoch [3/5], Step [200/212], Loss: 2.9521, Accuracy: 0.1900\n",
    "Epoch [4/5], Step [100/212], Loss: 3.0117, Accuracy: 0.1900\n",
    "Epoch [4/5], Step [200/212], Loss: 2.7980, Accuracy: 0.2200\n",
    "Epoch [5/5], Step [100/212], Loss: 2.7254, Accuracy: 0.2200\n",
    "Epoch [5/5], Step [200/212], Loss: 2.8732, Accuracy: 0.1900"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
